{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5EWu0XOE39LrASO0r1PEU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenithaSri/SPEAK/blob/main/Speak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7V0yr295hy-o",
        "outputId": "301b5c85-b6b3-4d9c-8ab5-16679192813d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/800.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m716.8/800.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio transformers openai-whisper --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import whisper\n",
        "from transformers import pipeline\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pKQKEPXzh7aD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load once\n",
        "emotion_classifier = pipeline(\"audio-classification\", model=\"superb/hubert-large-superb-er\")\n",
        "whisper_model = whisper.load_model(\"base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7CCOzeUeh9mv",
        "outputId": "2520f9f7-8937-48d3-81cc-68bd6589eaef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_emotion_chart(labels, scores):\n",
        "    emoji_map = {\n",
        "        \"hap\": \"😊 Happy\", \"sad\": \"😔 Sad\", \"neu\": \"😐 Neutral\",\n",
        "        \"ang\": \"😠 Angry\", \"fea\": \"😨 Fear\", \"dis\": \"🤢 Disgust\", \"sur\": \"😮 Surprise\"\n",
        "    }\n",
        "    color_map = {\n",
        "        \"hap\": \"#facc15\", \"sad\": \"#60a5fa\", \"neu\": \"#a1a1aa\",\n",
        "        \"ang\": \"#ef4444\", \"fea\": \"#818cf8\", \"dis\": \"#14b8a6\", \"sur\": \"#f472b6\"\n",
        "    }\n",
        "\n",
        "    display_labels = [emoji_map.get(label, label) for label in labels]\n",
        "    colors = [color_map.get(label, \"#60a5fa\") for label in labels]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(5, 3.5))\n",
        "    bars = ax.barh(display_labels, scores, color=colors, edgecolor=\"black\", height=0.5)\n",
        "\n",
        "    for bar, score in zip(bars, scores):\n",
        "        ax.text(bar.get_width() + 0.02, bar.get_y() + bar.get_height() / 2,\n",
        "                f\"{score:.2f}\", va='center', fontsize=10, color='black')\n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_title(\"🎭 Emotion Confidence Scores\", fontsize=13, pad=10)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_facecolor(\"#f9fafb\")\n",
        "    fig.patch.set_facecolor(\"#f9fafb\")\n",
        "    for spine in ax.spines.values():\n",
        "        spine.set_visible(False)\n",
        "    ax.tick_params(axis='x', colors='gray')\n",
        "    ax.tick_params(axis='y', colors='gray')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def generate_next_moves(dominant_emotion, conf_score, transcript=\"\"):\n",
        "    suggestions = []\n",
        "\n",
        "    positive_tone_negative_words = False\n",
        "    harsh_words = [\"bad\", \"ugly\", \"terrible\", \"hate\", \"worst\"]\n",
        "    if 'happiness' in dominant_emotion:\n",
        "        for word in harsh_words:\n",
        "            if word in transcript.lower():\n",
        "                positive_tone_negative_words = True\n",
        "                break\n",
        "\n",
        "    if 'sadness' in dominant_emotion:\n",
        "        suggestions.append(\"Your tone feels low — try lifting the pitch slightly to bring more warmth.\")\n",
        "        suggestions.append(\"Even if the words are positive, a brighter tone helps convey enthusiasm.\")\n",
        "    elif 'happiness' in dominant_emotion and conf_score >= 80:\n",
        "        suggestions.append(\"Nice energy! Try modulating your tone even more for emphasis in key moments.\")\n",
        "        suggestions.append(\"Experiment with subtle emotional shifts as you speak for more depth.\")\n",
        "    elif 'neutral' in dominant_emotion:\n",
        "        suggestions.append(\"Add inflection to break a monotone pattern — especially at the ends of sentences.\")\n",
        "        suggestions.append(\"Highlight your message by stressing emotionally important words.\")\n",
        "    elif conf_score < 50:\n",
        "        suggestions.append(\"Try exaggerating vocal ups and downs when reading to unlock more expression.\")\n",
        "        suggestions.append(\"Slow down slightly and stretch certain words to vary your delivery.\")\n",
        "    else:\n",
        "        suggestions.append(\"Keep practicing tone variation — you’re building a solid base.\")\n",
        "\n",
        "    if positive_tone_negative_words:\n",
        "        suggestions.append(\"Your tone was upbeat, but the word choices were harsh — aim to align both for better impact.\")\n",
        "\n",
        "    return \"\\n- \" + \"\\n- \".join(suggestions)\n",
        "\n",
        "\n",
        "def generate_personacoach_report(emotions, transcript):\n",
        "    report = \"## 📝 **Your PersonaCoach Report**\\n\"\n",
        "    report += \"---\\n\\n\"\n",
        "\n",
        "    report += \"### 🗒️ **What You Said:**\\n\"\n",
        "    report += f\"> _{transcript.strip()}_\\n\\n\"\n",
        "\n",
        "    label_map = {\n",
        "        'hap': '😊 happiness', 'sad': '😔 sadness', 'neu': '😐 neutral',\n",
        "        'ang': '😠 anger', 'fea': '😨 fear', 'dis': '🤢 disgust', 'sur': '😮 surprise'\n",
        "    }\n",
        "    for e in emotions:\n",
        "        e['emotion'] = label_map.get(e['label'], e['label'])\n",
        "\n",
        "    scores = [s['score'] for s in emotions]\n",
        "    top_score = max(scores)\n",
        "    conf_score = int(top_score * 100)\n",
        "\n",
        "    meaningful_emotions = [(e['emotion'], e['score']) for e in emotions if e['score'] >= 0.2]\n",
        "    emotion_labels = [e[0] for e in meaningful_emotions]\n",
        "    dominant_emotion = emotion_labels[0] if emotion_labels else \"neutral\"\n",
        "\n",
        "    report += \"### 🎯 **Tone Strength:**\\n\"\n",
        "    report += f\"- Your tone scored **{conf_score}/100** in clarity.\\n\\n\"\n",
        "\n",
        "    report += \"### 🗣️ **Emotion & Delivery:**\\n\"\n",
        "    if meaningful_emotions:\n",
        "        emotions_str = \", \".join([f\"**{label}** ({score:.2f})\" for label, score in meaningful_emotions])\n",
        "        report += f\"- Emotionally, your voice showed: {emotions_str}\\n\"\n",
        "    else:\n",
        "        report += \"- Your tone wasn’t clearly expressive. Try reading with a bit more emphasis or emotion.\\n\"\n",
        "    report += \"\\n\"\n",
        "\n",
        "    filler_words = [\"um\", \"uh\", \"like\", \"you know\", \"so\", \"actually\", \"basically\", \"literally\"]\n",
        "    words = transcript.lower().split()\n",
        "    total_words = len(words)\n",
        "    filler_count = sum(words.count(fw) for fw in filler_words)\n",
        "    filler_ratio = filler_count / total_words if total_words > 0 else 0\n",
        "\n",
        "    report += \"### 💬 **Pausing Style (e.g., 'um', 'like', 'you know'):**\\n\"\n",
        "    report += f\"- You used **{filler_count}** hesitation phrases out of **{total_words}** words.\\n\"\n",
        "    if filler_ratio > 0.06:\n",
        "        report += \"- Try pausing instead of using fillers — it builds stronger presence.\\n\"\n",
        "    elif filler_ratio > 0.03:\n",
        "        report += \"- A few slipped in. Practice holding space with silence instead.\\n\"\n",
        "    else:\n",
        "        report += \"- Great fluency — you stayed focused and controlled.\\n\"\n",
        "    report += \"\\n\"\n",
        "\n",
        "    report += \"### ✅ **What You're Doing Well:**\\n\"\n",
        "    if top_score >= 0.75 and filler_ratio < 0.03:\n",
        "        report += \"- Confident tone and smooth delivery — keep it up!\\n\"\n",
        "    else:\n",
        "        report += \"- You’re on track. Keep refining tone and pacing.\\n\"\n",
        "\n",
        "    report += \"\\n### 🧭 **Next Moves:**\\n\"\n",
        "    report += generate_next_moves(dominant_emotion, conf_score, transcript) + \"\\n\"\n",
        "\n",
        "    return report\n"
      ],
      "metadata": {
        "id": "FWodKu0qh4oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(title=\"SPEAK: PersonaCoach\", theme=gr.themes.Soft()) as app:\n",
        "    # Header\n",
        "    gr.Markdown(\"\"\"\n",
        "    <div style=\"text-align:center; margin-bottom: 1rem;\">\n",
        "        <h1 style=\"font-size: 2.2rem; margin-bottom: 0.2rem;\">🎤 SPEAK: PersonaCoach</h1>\n",
        "        <p style=\"color: gray;\">Your smart voice reflection tool — assess tone, confidence, and delivery</p>\n",
        "    </div>\n",
        "    \"\"\", elem_id=\"header\")\n",
        "\n",
        "    # Audio Upload + Button\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=4):\n",
        "            audio_input = gr.Audio(type=\"filepath\", label=\"🎧 Upload Your Voice (.wav)\", elem_id=\"upload-audio\")\n",
        "        with gr.Column(scale=1, min_width=120):\n",
        "            analyze_btn = gr.Button(\"🔍 Analyze\", size=\"sm\", elem_id=\"analyze-btn\")\n",
        "\n",
        "    # Results Section\n",
        "    gr.Markdown(\"## 🧠 Results\", elem_id=\"results-header\")\n",
        "\n",
        "    with gr.Row(equal_height=True):\n",
        "        with gr.Column(scale=2):\n",
        "            feedback_output = gr.Markdown(label=\"📄 PersonaCoach Feedback\", elem_id=\"report-section\")\n",
        "        with gr.Column(scale=1):\n",
        "            emotion_plot = gr.Plot(label=\"🎭 Emotion Chart\", elem_id=\"chart\")\n",
        "\n",
        "    # Button click action\n",
        "    analyze_btn.click(\n",
        "        fn=analyze_audio,\n",
        "        inputs=audio_input,\n",
        "        outputs=[gr.Textbox(visible=False), emotion_plot, feedback_output]\n",
        "    )\n",
        "\n",
        "app.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "BooVuC8Dprix",
        "outputId": "ba85c3e5-f1c1-4e60-8cf4-333598b3ec4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://713523b2452e45ec77.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://713523b2452e45ec77.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}